{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing and Feature Extraction for the Berlin Database of Emotional Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook handles the preprocessing pipeline\n",
    "It includes:\n",
    "1.  **Data Loading & Statistics**: Analyzing file durations and sample rates.\n",
    "2.  **Data Cleaning**: Removing silence, normalizing, and padding audio.\n",
    "3.  **Feature Extraction**: Computing Mel Spectrograms, MFCCs, and other spectral/prosodic features.\n",
    "4.  **Visualization**: Visualizing various feature representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa as lr\n",
    "import h5py\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import re\n",
    "import wave\n",
    "import scipy\n",
    "from scipy import interpolate\n",
    "from scipy.stats import zscore\n",
    "import scipy.stats\n",
    "from scipy.signal import firwin, lfilter, savgol_filter\n",
    "from scipy.fftpack import dct\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torchaudio.transforms import Vad\n",
    "import torchaudio.compliance.kaldi as ta_kaldi\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pywt\n",
    "from pydub import AudioSegment\n",
    "import datetime \n",
    "import python_speech_features\n",
    "import librosa.display\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "data_base_path = os.path.abspath(os.path.join(current_dir, \"../../../data\"))\n",
    "\n",
    "audio_data_path = os.path.join(data_base_path, \"EmoDB/wav/\")\n",
    "silb_data_path = os.path.join(data_base_path, \"EmoDB/silb/\")\n",
    "audio_data_path_with_silence = os.path.join(data_base_path, \"EmoDB/BerlinEMO_DB_Silence/\")\n",
    "usm_data_path = os.path.join(data_base_path, \"usm_eval/\")\n",
    "\n",
    "if not os.path.exists(audio_data_path_with_silence):\n",
    "    os.makedirs(audio_data_path_with_silence)\n",
    "\n",
    "print(f\"Audio Data Path: {audio_data_path}\")\n",
    "print(f\"Output Path: {audio_data_path_with_silence}\")\n",
    "      \n",
    "usm_classes=[\"airplane\",\n",
    "\"alarm\",\n",
    "\"birds\",\n",
    "\"bus\",\n",
    "\"car\", \n",
    "\"cheering\",\n",
    "\"church bell\",\n",
    "\"dogs\",\n",
    "\"drilling\",\n",
    "\"glass break\",\n",
    "\"gunshot\",\n",
    "\"hammer\",\n",
    "\"helicopter\",\n",
    "\"jackhammer\",\n",
    "\"lawn mower\",\n",
    "\"motorcycle\",\n",
    "\"music\",\n",
    "\"rain\",\n",
    "\"sawing\",\n",
    "\"scream\",\n",
    "\"siren\",\n",
    "\"speech\",\n",
    "\"thunderstorm\",\n",
    "\"train\",\n",
    "\"truck\",\n",
    "\"wind\"]\n",
    "\n",
    "selected_classes=[\"wind\", \"thunderstorm\", \"rain\", \"bus\", \"birds\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class and Sentence Mapping\n",
    "Helper functions to extract emotion labels and sentence text from the specific filename format of Emo-DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(filename):\n",
    "    if \"W\" in filename[5]:\n",
    "        return \"Anger\"\n",
    "    if \"L\" in filename[5]:\n",
    "        return \"Boring\"\n",
    "    if \"E\" in filename[5]:\n",
    "        return \"Disgust\"\n",
    "    if \"A\" in filename[5]:\n",
    "        return \"Fear\"\n",
    "    if \"F\" in filename[5]:\n",
    "        return \"Happy\"\n",
    "    if \"T\" in filename[5]:\n",
    "        return \"Sad\"\n",
    "    if \"N\" in filename[5]:\n",
    "        return \"Neutral\"\n",
    "\n",
    "\n",
    "def get_sentence(filename):\n",
    "    if \"a01\" in filename[2:5]:\n",
    "        return \"Der Lappen liegt auf dem Eisschrank.\"\n",
    "    if \"a02\" in filename[2:5]:\n",
    "        return \"Das will sie am Mittwoch abgeben.\"\n",
    "    if \"a04\" in filename[2:5]:\n",
    "        return \"Heute abend könnte ich es ihm sagen.\"\n",
    "    if \"a05\" in filename[2:5]:\n",
    "        return \"Das schwarze Stück Papier befindet sich da oben neben dem Holzstück.\"\n",
    "    if \"a07\" in filename[2:5]:\n",
    "        return \"In sieben Stunden wird es soweit sein.\"\n",
    "    if \"b01\" in filename[2:5]:\n",
    "        return \"Was sind denn das für Tüten, die da unter dem Tisch stehen?\"\n",
    "    if \"b02\" in filename[2:5]:\n",
    "        return \"Sie haben es gerade hochgetragen und jetzt gehen sie wieder runter.\"\n",
    "    if \"b03\" in filename[2:5]:\n",
    "        return \"An den Wochenenden bin ich jetzt immer nach Hause gefahren und habe Agnes besucht.\"\n",
    "    if \"b09\" in filename[2:5]:\n",
    "        return \"Ich will das eben wegbringen und dann mit Karl was trinken gehen.\"\n",
    "    if \"b10\" in filename[2:5]:\n",
    "        return \"Die wird auf dem Platz sein, wo wir sie immer hinlegen.\t\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Statistics\n",
    "We iterate through all `.wav` files to calculate the distribution of audio durations and verify the sample rate. We also calculate the average decibel level for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_stats(audio_data_path):\n",
    "    file_names=[]\n",
    "    samples=[]\n",
    "    durations=[]\n",
    "    for path in os.listdir(audio_data_path):\n",
    "        if path[-3:]==\"wav\":\n",
    "            file_path=os.path.join(audio_data_path, path)\n",
    "            file_names.append(path)\n",
    "            y, sr = sf.read(file_path)\n",
    "            # print(sr)\n",
    "            durations.append((len(y)/sr))\n",
    "            samples.append(y)\n",
    "    max_duration=np.max(np.array(durations))\n",
    "    print(f'Es sind {len(file_names)} Dateien geladen worden.')\n",
    "    print(f'Die Längste Datei ist {np.max(np.array(durations)):.3f} Sekunden und die kürzeste {np.min(np.array(durations)):.3f} Sekunden lang. Durchschnittlich sind die Dateien {np.mean(np.array(durations)):.3f} Sekunden lang.')\n",
    "    print(f'Die Samplerate ist {sr}.')\n",
    "    return samples, file_names, durations\n",
    "\n",
    "def calculate_avg_db(audio_data_path):\n",
    "    samples, _, _ = database_stats(audio_data_path)\n",
    "    avg_db = [np.mean(np.abs(y)) for y in samples]\n",
    "    avg_db= np.mean(np.array(avg_db))\n",
    "    avg_db_db = 20 * np.log10(avg_db)\n",
    "    print(f\"Die durchschnittliche Lautstärke beträgt {avg_db_db:.3f} dB.\")\n",
    "    return avg_db_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, file_names, durations=database_stats(audio_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Distribution\n",
    "We count the occurrences of each emotion category (Anger, Boredom, Disgust, Fear, Happy, Sad, Neutral) to check dataset balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_emotions(filenames):\n",
    "    anger, boring, disgust, fear, happy, sad, neutral = 0,0,0,0,0,0,0\n",
    "    for filename in filenames:\n",
    "        if \"W\" in filename[5]:\n",
    "            anger+=1\n",
    "        if \"L\" in filename[5]:\n",
    "            boring+=1\n",
    "        if \"E\" in filename[5]:\n",
    "            disgust+=1\n",
    "        if \"A\" in filename[5]:\n",
    "            fear+=1\n",
    "        if \"F\" in filename[5]:\n",
    "            happy+=1\n",
    "        if \"T\" in filename[5]:\n",
    "            sad+=1\n",
    "        if \"N\" in filename[5]:\n",
    "            neutral+=1\n",
    "    return np.array([disgust, anger, fear, happy, neutral, boring, sad])\n",
    "\n",
    "_, file_names, _ = database_stats(audio_data_path)\n",
    "list_count=count_emotions(file_names)\n",
    "print(f\"Emotion classes: [E_n={list_count[0]}, W_n={list_count[1]}, A_n={list_count[2]}, F_n={list_count[3]}, N_n={list_count[4]}, L_n={list_count[5]}, T_n={list_count[6]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Functions\n",
    "Functions to visualize Waveforms, Log Mel Spectrograms, MFCCs, Tonnetz, and Spectral Contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_or_save_waveforms(y,y_emphasized, task, path, folder_path_to_save):\n",
    "    file_name_wave = folder_path_to_save+f\"{path[:-4]}_wave.png\"\n",
    "    time1 = lr.times_like(y, sr=sr)\n",
    "    time2 = lr.times_like(y_emphasized, sr=sr)\n",
    "    plt.figure()\n",
    "    plt.plot(time1, y, color='midnightblue', label='Signal')  \n",
    "    plt.plot(time2, y_emphasized, color='cornflowerblue', label='Emphasized Signal', alpha=0.6) \n",
    "    plt.tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False)\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.title(f\"Waveforms of {path[:-4]}\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    if task==\"plot\":\n",
    "        plt.show()\n",
    "    elif task==\"save\":\n",
    "        plt.savefig(file_name_wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log_mel_spectrogram(mel_spec_log, y, sr, path, folder_path_to_save, task):\n",
    "    sentence = get_sentence(path[:-4])\n",
    "    emotion_class = get_class(path[:-4])\n",
    "    font_settings = {\n",
    "                'family': 'sans-serif',\n",
    "                'size': 16,\n",
    "                'weight': 'normal',\n",
    "            }\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mel_spec_log, origin='lower', aspect='auto', extent=[0, len(y)/sr, 0, sr/2]) # cmap='Spectral'\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(\"Log Mel Frequency Spectrogram\", fontdict=font_settings)\n",
    "    plt.xlabel('Time in s', fontdict=font_settings)\n",
    "    plt.ylabel('Frequency in Hz', fontdict=font_settings)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if task == \"plot\":\n",
    "        plt.show()\n",
    "    elif task == \"save\":\n",
    "        plt.savefig(folder_path_to_save + f\"{path[:-4]}_log_mel_spectrogram.png\")\n",
    "\n",
    "\n",
    "def plot_mfcc(mfcc, y, sr, n_mfcc, path, folder_path_to_save, task):\n",
    "    utterance = get_sentence(path[:-4])\n",
    "    emotion_class = get_class(path[:-4])\n",
    "    font_settings = {\n",
    "                'family': 'sans-serif',\n",
    "                'size': 16,\n",
    "                'weight': 'normal',\n",
    "            }\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mfcc, aspect='auto', extent=[0, len(y)/sr, 0, n_mfcc], origin='lower', cmap=\"Spectral\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(\"Mel Frequency Cepstral Coefficients\", fontdict=font_settings)\n",
    "    plt.ylabel('Coefficient', fontdict=font_settings)\n",
    "    plt.xlabel('Time in s', fontdict=font_settings)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if task == \"plot\":\n",
    "        plt.show()\n",
    "    elif task == \"save\":\n",
    "        plt.savefig(folder_path_to_save + f\"{path[:-4]}_mfcc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tonnetz(tonnetz):\n",
    "    fifths_octaves = tonnetz[0:2, :]\n",
    "    minor_thirds_octaves = tonnetz[2:4, :]\n",
    "    major_thirds_octaves = tonnetz[4:6, :]\n",
    "    def plot_tonal_space(x, y, title):\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.scatter(x, y)\n",
    "        plt.xlim([-1, 1])\n",
    "        plt.ylim([-1, 1])\n",
    "        plt.title(title)\n",
    "        plt.xlabel('dimension 1')\n",
    "        plt.ylabel('dimension 2')\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "    # plot dimensions\n",
    "    plot_tonal_space(fifths_octaves[0], fifths_octaves[1], 'fifths and octaves')\n",
    "    plot_tonal_space(minor_thirds_octaves[0], minor_thirds_octaves[1], 'minor thirds and octaves')\n",
    "    plot_tonal_space(major_thirds_octaves[0], major_thirds_octaves[1], 'major thirds and octaves')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "def plot_spectral_contrast(spectral_contrast, hop_length, sr):\n",
    "    times = np.arange(spectral_contrast.shape[1]) * hop_length / sr  \n",
    "    for i, band in enumerate(spectral_contrast, 1):\n",
    "        plt.plot(times, band, label=f'contrast band {i}')\n",
    "    plt.xlabel('time in seconds')\n",
    "    plt.ylabel('spectral contrast (intensity?)')\n",
    "    plt.title('spectral contrast (time)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_standard(input, name):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.imshow(input, aspect='auto', origin='lower', cmap='coolwarm')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Visualization\n",
    "\n",
    "### Visualizations of Mel Frequency Cepstral Coefficients and Log Mel Frequency Spectrograms\n",
    "\n",
    "We run a test extraction on a single file to visualize Mel Spectrograms and MFCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_to_save = \"./VisualsEMO_DB/\"\n",
    "if os.path.isdir(folder_path_to_save):\n",
    "    shutil.rmtree(folder_path_to_save)\n",
    "os.makedirs(folder_path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_plot_visualization(x, task, n_fft=440, n_mfcc=13, hop_length=160, n_mels=128, pre_coef=0.96):\n",
    "    file_names=[]\n",
    "    i=1\n",
    "    for path in os.listdir(audio_data_path):\n",
    "        if path[-3:]==\"wav\":\n",
    "            file_path=os.path.join(audio_data_path, path)\n",
    "            file_names.append(path)\n",
    "            y, sr = lr.load(file_path)\n",
    "            y_emphasized=lr.effects.preemphasis(y, coef=pre_coef)\n",
    "            # y_emphasized = np.append(y[0], y[1:] - pre_emphasis * y[:-1])\n",
    "            stft = lr.stft(y_emphasized, n_fft=n_fft, hop_length=hop_length)\n",
    "            mel_spec = lr.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, norm=2, window='hamming')\n",
    "            mel_spec_log = np.array(lr.power_to_db(mel_spec, ref=np.max).astype(np.float32))\n",
    "            mfcc = lr.feature.mfcc(S=lr.power_to_db(mel_spec), n_mfcc=n_mfcc)\n",
    "            plot_log_mel_spectrogram(mel_spec_log,y,sr, path, folder_path_to_save, task)\n",
    "            print(mfcc.shape)\n",
    "            plot_mfcc(mfcc, y, sr, n_mfcc, path, folder_path_to_save, task)\n",
    "            if x>=i:\n",
    "                i+=1\n",
    "            else: break\n",
    "create_or_plot_visualization(x=1, task=\"plot\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Class\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "This class handles:\n",
    "1.  **Silence Removal**: Removing silence from audio clips.\n",
    "2.  **Normalization**: Normalizing audio volume to a target dB.\n",
    "3.  **Augmentation**: Options for White Noise and USM dataset augmentation.\n",
    "4.  **Saving**: Saving the processed files to a new directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Preprocessing:\n",
    "    def __init__(self, sr, origin_folder_path, audio_data_path_with_silence, usm_data_path, usm_classes, selected_classes, avg_db):\n",
    "        self.origin_folder_path = origin_folder_path\n",
    "        self.audio_data_path_with_silence = audio_data_path_with_silence\n",
    "        self.usm_data_path = usm_data_path\n",
    "        self.usm_classes = usm_classes\n",
    "        self.selected_classes = selected_classes\n",
    "        self.file_list = os.listdir(self.origin_folder_path)\n",
    "        self.y_usm_array, self.sr_usm = self.get_y_usm()\n",
    "        self.sr_usm=44100\n",
    "        self.target_sr=16000\n",
    "        self.sr=sr\n",
    "        self.avg_db=avg_db\n",
    "        self.arb_length=4 # in seconds\n",
    "        self.pre_emp=0.97\n",
    "\n",
    "    def get_y_usm(self):\n",
    "        # Load USM Datensatz\n",
    "        samples_usm = []\n",
    "        for i in range(0,2000):\n",
    "            path=f\"{i}_mix.wav\"\n",
    "            path2=f\"{i}_mix_target.npy\"\n",
    "            file_path=os.path.join(self.usm_data_path, path)\n",
    "            file_path2=os.path.join(self.usm_data_path, path2)\n",
    "            data = np.load(file_path2, allow_pickle=True)\n",
    "            if self.usm_classes[np.argmax(data)] in self.selected_classes:\n",
    "                # print(self.usm_classes[np.argmax(data)])\n",
    "                y_usm=[]\n",
    "                y_usm, sr_usm = lr.load(file_path, sr=None)\n",
    "                samples_usm.append(y_usm)\n",
    "        return samples_usm, sr_usm\n",
    "    \n",
    "\n",
    "    def pitch_shift(self, samples_filtered, sr):\n",
    "        steps = random.uniform(-4, 4)\n",
    "        samples_filtered = lr.effects.pitch_shift(samples_filtered, sr=sr, n_steps=steps)\n",
    "        return samples_filtered\n",
    "    \n",
    "\n",
    "    def resample(self, y, orig_sr, target_sr):\n",
    "        y_resample = lr.resample(y.T, orig_sr, target_sr)\n",
    "        if len(y_resample.shape) > 1:\n",
    "            y_resample = np.average(y_resample, axis=0)\n",
    "        return y_resample, sr\n",
    "\n",
    "\n",
    "    def da_white(self, y, sr):\n",
    "        # Normalverteilung für White Noise\n",
    "        white_noise = random.uniform(0.001,0.015) * np.random.rand(len(y))\n",
    "        samples_white = y + white_noise\n",
    "        return samples_white\n",
    "    \n",
    "\n",
    "    def da_usm(self, y, sr):\n",
    "        # USM Datensatz Augmentierung\n",
    "        random_sample_idx = random.randrange(len(self.y_usm_array))\n",
    "        y_usm=[]\n",
    "        y_usm = self.y_usm_array[random_sample_idx]\n",
    "        y_usm, sr_usm = self.resample(y_usm, self.sr_usm, self.sr)\n",
    "        start = int(random.uniform(0, 1) * (len(y_usm) - len(y)))\n",
    "        clipped_y = y_usm[start:(start + len(y))]\n",
    "        # Aktuell wird keine Audio Datei aus dem USM Datensatz genutzt, wenn die Audio Datei länger als 5 Sekunden ist (maximale Länge der USM Clips)\n",
    "        if clipped_y.shape != y.shape:\n",
    "            return y\n",
    "        y += clipped_y\n",
    "        return y    \n",
    "            \n",
    "\n",
    "    def add_silence_noise_and_save(self):\n",
    "        idx=0\n",
    "        for file in self.file_list:\n",
    "            file_path = os.path.join(self.origin_folder_path, file)\n",
    "            target_file_path= os.path.join(self.audio_data_path_with_silence, file)\n",
    "\n",
    "            if os.path.isfile(file_path) and file_path[-3:]==\"wav\":\n",
    "                y, sr = sf.read(file_path)\n",
    "                # for resampling\n",
    "                # y,sr=self.resample(y, self.sr, self.target_sr)\n",
    "                # self.sr=self.target_sr\n",
    "                if idx==0: print(sr)\n",
    "                \n",
    "                # set arbritary length of clips\n",
    "                arb_length=self.arb_length\n",
    "                if len(y)/sr>arb_length:\n",
    "                    num_chunks = len(y) // (arb_length*sr)\n",
    "                    for i in range(num_chunks):\n",
    "                        chunk = y[i*arb_length*sr : (i+1)*arb_length*sr]\n",
    "                        self.process_and_save(arb_length, str(i), chunk, sr, target_file_path)\n",
    "                        idx += 1\n",
    "                else:\n",
    "                    self.process_and_save(arb_length, '', y, sr, target_file_path)\n",
    "                    idx += 1\n",
    "    \n",
    "\n",
    "    def normalize(self, y):\n",
    "        avg_amplitude_y = np.mean(np.abs(y))\n",
    "        if avg_amplitude_y > 0:\n",
    "            vol_y_db = 20 * np.log10(avg_amplitude_y)\n",
    "        else:\n",
    "            vol_y_db = 0 \n",
    "        db_diff = self.avg_db - vol_y_db\n",
    "        scaling_factor = 10 ** (db_diff / 20)\n",
    "        y_norm = y * scaling_factor\n",
    "        return y_norm\n",
    "    \n",
    "\n",
    "    def process_and_save(self, arb_length, i, y, sr, target_file_path):\n",
    "        if i!='':\n",
    "            target_file_path=target_file_path[:-4]+\"_\"+str(i)+\".wav\"\n",
    "        # high pass filter with firwin\n",
    "        y = self.normalize(y)\n",
    "        y=np.append(y[0], y[1:] - self.pre_emp * y[:-1])\n",
    "        # y = savgol_filter(y, 9, 2)\n",
    "        # overlay white noise and sounds from the USM dataset\n",
    "        # y = da_usm(y,sr) and da_white(y,sr) to augment data\n",
    "        # z-score normalization\n",
    "        # y = zscore(y)\n",
    "        # pre emphasis\n",
    "        # y = lr.effects.preemphasis(y, coef=0.97)\n",
    "        # silence padding before and behind\n",
    "        right_array_length = int((sr*arb_length-len(y)))\n",
    "        right_array = np.zeros(right_array_length)\n",
    "        samples = np.concatenate((y,right_array), axis=0)\n",
    "        sf.write(target_file_path, samples, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Output Directory\n",
    "\n",
    "### Remove and create folder for the data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(audio_data_path_with_silence)\n",
    "os.makedirs(audio_data_path_with_silence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Preprocessing\n",
    "### Run Preprocessing\n",
    "Instantiate the preprocessing class and run the pipeline to clean and prepare the audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_db=calculate_avg_db(audio_data_path)\n",
    "processor = Data_Preprocessing(sr, audio_data_path, audio_data_path_with_silence, usm_data_path, usm_classes, selected_classes, avg_db)\n",
    "processor.add_silence_noise_and_save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load function to pad the audio sequence by https://github.com/Jiaxin-Ye/TIM-Net_SER/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(signal, sr, mean_signal_length=96000): # avg signal Emo-DB: 2,78 seconds\n",
    "    if len(signal.shape) > 1:\n",
    "        signal = np.average(signal, axis=0)\n",
    "    s_len = len(signal)\n",
    "    if s_len < mean_signal_length:\n",
    "        pad_len = mean_signal_length - s_len\n",
    "        pad_rem = pad_len % 2\n",
    "        pad_len //= 2\n",
    "        signal = np.pad(signal, (pad_len, pad_len + pad_rem), 'constant', constant_values = 0)\n",
    "    else:\n",
    "        pad_len = s_len - mean_signal_length\n",
    "        pad_len //= 2\n",
    "        signal = signal[pad_len:pad_len + mean_signal_length]\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Domain Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Features\n",
    "Function to compute statistical measures (mean, std, min, max, skew, kurtosis, etc.) from feature arrays.\n",
    "\n",
    "Adapted from https://maelfabien.github.io/machinelearning/Speech9/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(freqs):\n",
    "    mean = np.mean(freqs)\n",
    "    std = np.std(freqs) \n",
    "    min = np.min(freqs)\n",
    "    max = np.max(freqs)\n",
    "    maxv = np.amax(freqs) \n",
    "    minv = np.amin(freqs) \n",
    "    median = np.median(freqs)\n",
    "    cov=np.cov(freqs)\n",
    "    var=np.var(freqs)\n",
    "    rms=np.sqrt(np.mean(np.square(freqs)))\n",
    "    skew = scipy.stats.skew(freqs)\n",
    "    kurt = scipy.stats.kurtosis(freqs)\n",
    "    q1 = np.quantile(freqs, 0.25)\n",
    "    q2 = np.quantile(freqs, 0.5)\n",
    "    q3 = np.quantile(freqs, 0.75)\n",
    "    mode = scipy.stats.mode(freqs)[0][0]\n",
    "    iqr = scipy.stats.iqr(freqs)\n",
    "    return [min, max, mean, median, mode, std, cov, var, rms, q1, q2, q3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Feature Visualization\n",
    "### Comprehensive Feature Test\n",
    "Testing a wide range of feature extraction methods (CQT, VQT, Chromagram, Spectral Contrast, Tonnetz) to explore potential features for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find suitable parameters (Paper suggested: n_fft should be between 20 and 30 ms with an overlap of 30% to 50%)\n",
    "def experimental_feature_visualization(audio_data_path, x=1, sr=16000, n_fft=400, hop_length=200, n_mels=128, n_mfcc=40, fmin_pitch=lr.note_to_hz('A2'), fmax_pitch=lr.note_to_hz('A7'), fmin=800, fmax=8000, bins_per_octave=12, pre_emp=0.96):\n",
    "    idx=1\n",
    "    n_bins= int(np.log2(fmax / fmin) * bins_per_octave)-5\n",
    "    for file in os.listdir(audio_data_path):\n",
    "        file_path = os.path.join(audio_data_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_path[-3:]==\"wav\":\n",
    "                y, sr = sf.read(file_path)\n",
    "                print(f\"sr={sr}\")\n",
    "                y_pre = np.append(y[0], y[1:] - pre_emp * y[:-1])\n",
    "                window = scipy.signal.windows.general_hamming(n_fft, alpha=0.46)\n",
    "                mel = lr.feature.melspectrogram(y=y_pre, sr=sr, n_fft=n_fft, fmin=fmin, fmax=fmax, hop_length=hop_length, n_mels=n_mels, norm=2, window=window)\n",
    "                mel=lr.amplitude_to_db(mel, ref=np.max)\n",
    "                mfcc = lr.feature.mfcc(y=y, S=mel, n_mfcc=n_mfcc)\n",
    "                cqt = lr.cqt(y, sr=sr, n_bins=n_bins, fmin=fmin, bins_per_octave=bins_per_octave, hop_length=hop_length)\n",
    "                log_cqt = lr.amplitude_to_db(np.abs(cqt), ref=np.max)  \n",
    "                spectral_contrast = lr.feature.spectral_contrast(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "                vqt = lr.vqt(y=y, sr=sr, hop_length=hop_length, fmin=fmin, n_bins=n_bins, bins_per_octave=bins_per_octave, gamma=0)\n",
    "                chromagram = lr.feature.chroma_cqt(C=cqt, sr=sr, n_chroma=12)\n",
    "                delta_mfccs = lr.feature.delta(mel)\n",
    "                delta2_mfccs = lr.feature.delta(mel, order=2)\n",
    "                plot_standard(log_cqt, 'constant-q power spectrum')\n",
    "                pitches = lr.yin(y, sr=sr, hop_length=hop_length, fmin=fmin_pitch, fmax=fmax_pitch)            \n",
    "                tonnetz = lr.feature.tonnetz(y=y, sr=sr, chroma=vqt, hop_length=hop_length, n_chroma=bins_per_octave, n_octaves=5)\n",
    "                plot_tonnetz(tonnetz)\n",
    "                plot_spectral_contrast(spectral_contrast, hop_length, sr)\n",
    "                plot_standard(lr.amplitude_to_db(vqt, ref=np.max), 'Variable-Q Transform (VQT)')\n",
    "                plot_standard(lr.amplitude_to_db(cqt, ref=np.max), 'Constant-Q Transform (CQT)')\n",
    "                plot_standard(np.abs(chromagram), 'Chromagram')\n",
    "                mel=np.mean(a=mel, axis=1)\n",
    "                cqt=np.mean(a=cqt, axis=1)\n",
    "                log_cqt=np.mean(a=log_cqt, axis=1)\n",
    "                vqt=np.mean(a=vqt.real, axis=1)\n",
    "                mfcc=np.mean(a=mfcc, axis=1)\n",
    "                spectral_contrast=np.mean(a=spectral_contrast, axis=1)\n",
    "                delta_mfccs=np.mean(a=delta_mfccs, axis=1)\n",
    "                delta2_mfccs=np.mean(a=delta2_mfccs, axis=1)\n",
    "                tonnetz=np.mean(a=tonnetz.real, axis=1)\n",
    "                chromagram=np.mean(a=chromagram.real, axis=1)\n",
    "                mixed_features=np.hstack((delta_mfccs, delta2_mfccs, mfcc, spectral_contrast, tonnetz, chromagram, delta_mfccs, mel, cqt, log_cqt))\n",
    "                mixed_features = np.array(torch.from_numpy(mixed_features).unsqueeze(1))\n",
    "                print(f\"pitches yin shape: {pitches.shape}\")\n",
    "                print(f\"mel: {mel.shape}\")\n",
    "                print(f\"mfccs: {mfcc.shape}\")\n",
    "                print(f\"delta_mfccs: {delta_mfccs.shape}\")\n",
    "                print(f\"delta_delta_mfccs: {delta2_mfccs.shape}\")\n",
    "                print(f\"log_cqt: {log_cqt.shape}\")\n",
    "                print(f\"spectral_contrast: {spectral_contrast.shape}\")\n",
    "                print(f\"chromagram: {chromagram.shape}\")\n",
    "                print(f\"chroma: {chromagram.shape}\")\n",
    "                print(f\"tonnetz: {tonnetz.shape}\")\n",
    "                print(f\"mixed_features: {mixed_features.shape}\")\n",
    "                if idx==x:\n",
    "                    break\n",
    "                idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_feature_visualization(x=1, audio_data_path=audio_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prosodic Feature Extraction\n",
    "### Prosodic Features\n",
    "Extracting Pitch (F0), RMS Energy, and Zero Crossing Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prosodic_features(y, sr, n_fft, hop_length, norm, n_mfcc=40, n_mels=40, pre_emp=0.97, fmin_pitch=lr.note_to_hz('A2'), fmax_pitch=lr.note_to_hz('A7')):\n",
    "    try:\n",
    "        y = np.append(y[0], y[1:] - pre_emp * y[:-1])\n",
    "        f0 = lr.yin(y, sr=sr, hop_length=hop_length, fmin=fmin_pitch, fmax=fmax_pitch)  \n",
    "        # print(f0.shape)      \n",
    "        # pitches = np.array(torch.from_numpy(pitches).unsqueeze(1))\n",
    "        rms_energy = librosa.feature.rms(y=y, frame_length=n_fft, hop_length=hop_length)[0]\n",
    "        rms_energy = np.array(torch.from_numpy(rms_energy).unsqueeze(1))\n",
    "        zcr = lr.feature.zero_crossing_rate(y=y, frame_length=n_fft, hop_length=hop_length)\n",
    "        zcr = np.array(torch.from_numpy(zcr).permute(1,0))\n",
    "        if norm==\"zscore\":\n",
    "            pitch = zscore(f0)\n",
    "            rms_energy = zscore(rms_energy)\n",
    "            zcr = zscore(zcr)\n",
    "        # print(zcr.shape)\n",
    "        # print(rms_energy.shape)\n",
    "        prosodic_features = np.concatenate((zcr, rms_energy), axis=1)\n",
    "        prosodic_features = np.array(torch.from_numpy(prosodic_features).permute(1,0))\n",
    "    except Exception as e:\n",
    "        print(\"none\",e)\n",
    "        return None \n",
    "    return prosodic_features, f0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Feature Extraction\n",
    "### Spectral Features\n",
    "Extracting comprehensive spectral features including MFCCs (and deltas), Spectral Contrast, Centroid, Bandwidth, Rolloff, and Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectral_features(y, sr, n_fft, hop_length, norm, n_mfcc=40, n_mels=40, pre_emp=0.97, fmin=20, fmax=int(sr/2), bins_per_octave=12, fmin_pitch=lr.note_to_hz('A2'), fmax_pitch=lr.note_to_hz('A7')):\n",
    "    try:\n",
    "        y = np.append(y[0], y[1:] - pre_emp * y[:-1])\n",
    "        mfccs = lr.feature.mfcc(y=y, sr=sr,  n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        mfccs_d = lr.feature.delta(mfccs)\n",
    "        mfccs_2d = lr.feature.delta(mfccs, order=2)\n",
    "        spectral_contrast = lr.feature.spectral_contrast(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "        spectral_centroids = lr.feature.spectral_centroid(y=(y+0.001), sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "        spectral_bandwidth = lr.feature.spectral_bandwidth(y=(y+0.001), sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "        spectral_rolloff = lr.feature.spectral_rolloff(y=(y+0.001), sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "        window = scipy.signal.windows.general_hamming(n_fft, alpha=0.46)\n",
    "        mel = lr.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, norm=2, window=window) \n",
    "        log_mel = lr.amplitude_to_db(mel, ref=np.max)\n",
    "        chromagram = lr.feature.chroma_cqt(y=y, sr=sr, hop_length=hop_length)\n",
    "        tonnetz = lr.feature.tonnetz(y=lr.effects.harmonic(y), sr=sr, hop_length=hop_length)\n",
    "        n_bins= int(np.log2(fmax / fmin)* bins_per_octave)-5\n",
    "        cqt = lr.cqt(y, sr=sr, n_bins=n_bins, fmin=fmin, bins_per_octave=bins_per_octave, hop_length=hop_length)\n",
    "        log_cqt = lr.amplitude_to_db(np.abs(cqt), ref=np.max)  \n",
    "        cqcc = lr.feature.mfcc(S=log_cqt, sr=sr, n_mfcc=n_mfcc, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "        if norm==\"zscore\":\n",
    "            cqcc = zscore(cqcc, axis=1)\n",
    "            tonnetz = zscore(tonnetz, axis=1)\n",
    "            log_mel = zscore(log_mel, axis=1)\n",
    "            mfccs = zscore(mfccs, axis=1)\n",
    "            mfccs_d = zscore(mfccs_d, axis=1)\n",
    "            mfccs_2d = zscore(mfccs_2d, axis=1)\n",
    "            spectral_contrast = zscore(spectral_contrast, axis=1)\n",
    "            spectral_centroids = zscore(spectral_centroids, axis=1)\n",
    "            spectral_rolloff = zscore(spectral_rolloff, axis=1)\n",
    "            spectral_bandwidth = zscore(spectral_bandwidth, axis=1)\n",
    "            chromagram = zscore(chromagram, axis=1)\n",
    "        spectral_features = np.concatenate((cqcc, tonnetz, log_mel, mfccs, mfccs_d, mfccs_2d, spectral_contrast, spectral_centroids, spectral_rolloff, spectral_bandwidth, chromagram), axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"none\",e)\n",
    "        return None \n",
    "    return spectral_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel Frequency Cepstral Coeffients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(y, sr, n_fft, hop_length,  n_mfcc, window_size=400, pre_emp=0.97, n_mels=84, fmin=20,fmax=300, fmin_pitch=lr.note_to_hz('A2'), fmax_pitch=lr.note_to_hz('A7')):\n",
    "    try:\n",
    "        # optionally with padding\n",
    "        # y=padding(y,sr)\n",
    "        y=np.append(y[0], y[1:] - pre_emp * y[:-1])\n",
    "        # stft = lr.stft(y, n_fft=n_fft, hop_length=hop_length, win_length=window_size, window='hamming').real\n",
    "        mfcc = lr.feature.mfcc(y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length,  fmin=fmin, fmax=fmax, n_mels=n_mels, dct_type=2)\n",
    "        # print(mfcc.shape)\n",
    "        deltas=lr.feature.delta(mfcc)\n",
    "        deltasdeltas=lr.feature.delta(mfcc, order=2)\n",
    "        mfcc=zscore(mfcc)\n",
    "        deltas=zscore(deltas)\n",
    "        deltasdeltas=zscore(deltasdeltas)\n",
    "        # print(mfcc.shape, deltas.shape, deltasdeltas.shape)\n",
    "        mfcc_combined=np.concatenate([mfcc, deltas, deltasdeltas], axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"none\", e)\n",
    "        return None\n",
    "    return mfcc, mfcc_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(audio_data_path, n_mfcc=13, n_fft=445, hop_length=220):\n",
    "    for file in os.listdir(audio_data_path):\n",
    "        file_path = os.path.join(audio_data_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_path[-3:]==\"wav\":\n",
    "                y, sr = sf.read(file_path)\n",
    "                mfcc, mfcc_combined=get_mfcc(y,sr, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc)\n",
    "                features=mfcc\n",
    "                print(features.shape)\n",
    "                stats=[compute_stats(features[i, :]) for i in range(features.shape[0])] \n",
    "                features_combined=np.concatenate([stats, features], axis=1)\n",
    "                print(features_combined.shape)\n",
    "                plot_mfcc(features, y, sr, n_mfcc, file_path, folder_path_to_save, \"plot\")\n",
    "                break\n",
    "test(audio_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_dataset(dataset_name, audio_data_path, train_list, valid_list, test_list, n_fft, hop_length, n_mfcc=40, n_mels=128, fmin=100, fmax=10000):\n",
    "    def get_class(filename):\n",
    "        if \"W\" in filename[5]:\n",
    "            return 0\n",
    "        if \"L\" in filename[5]:\n",
    "            return 1\n",
    "        if \"E\" in filename[5]:\n",
    "            return 2\n",
    "        if \"A\" in filename[5]:\n",
    "            return 3\n",
    "        if \"F\" in filename[5]:\n",
    "            return 4\n",
    "        if \"T\" in filename[5]:\n",
    "            return 5\n",
    "        if \"N\" in filename[5]:\n",
    "            return 6\n",
    "    # assert dataset_name[-3:]!=\".h5\"\n",
    "    # dataset = h5py.File(dataset_name, mode=\"w\", libver=\"latest\")\n",
    "    train_dataset = h5py.File((\"train_\"+dataset_name), mode=\"w\", libver=\"latest\")\n",
    "    for file in train_list:\n",
    "        file_path = os.path.join(audio_data_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_path[-3:]==\"wav\":\n",
    "                y, sr = lr.load(file_path)\n",
    "                features, _ =get_mfcc(y,sr, n_fft=n_fft, hop_length=hop_length)\n",
    "                stats=[compute_stats(features[i, :]) for i in range(features.shape[0])] \n",
    "                features_combined=np.concatenate([stats, features], axis=1)\n",
    "                features_combined=np.array(torch.from_numpy(features_combined).permute(1,0))\n",
    "                filename = file.replace(\".wav\", \"_\")\n",
    "                file_class = get_class(filename)\n",
    "\n",
    "                f = [features_combined]\n",
    "                print(features_combined.shape)\n",
    "                idx=0\n",
    "                for _f in f:\n",
    "                    filename += str(idx)\n",
    "                    idx+=1\n",
    "                    h5spec = train_dataset.create_dataset(filename, data=_f)\n",
    "                    h5spec.attrs[\"class_label\"] = file_class\n",
    "                    # i+=1\n",
    "    train_dataset.close()\n",
    "    valid_dataset = h5py.File((\"valid_\"+dataset_name), mode=\"w\", libver=\"latest\")\n",
    "    for file in valid_list:\n",
    "        file_path = os.path.join(audio_data_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_path[-3:]==\"wav\":\n",
    "                y, sr = lr.load(file_path)\n",
    "                features, _ =get_mfcc(y,sr, n_fft=n_fft, hop_length=hop_length)\n",
    "                stats=[compute_stats(features[i, :]) for i in range(features.shape[0])] \n",
    "                features_combined=np.concatenate([stats, features], axis=1)\n",
    "                features_combined=np.array(torch.from_numpy(features_combined).permute(1,0))\n",
    "                filename = file.replace(\".wav\", \"_\")\n",
    "                file_class = get_class(filename)\n",
    "\n",
    "                f = [features_combined]\n",
    "                idx=0\n",
    "                print(features_combined.shape)\n",
    "                    \n",
    "                for _f in f:\n",
    "                    filename += str(idx)\n",
    "                    idx+=1\n",
    "                    h5spec = valid_dataset.create_dataset(filename, data=_f)\n",
    "                    h5spec.attrs[\"class_label\"] = file_class\n",
    "    valid_dataset.close()\n",
    "    test_dataset = h5py.File((\"test_\"+dataset_name), mode=\"w\", libver=\"latest\")\n",
    "    for file in test_list:\n",
    "        file_path = os.path.join(audio_data_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_path[-3:]==\"wav\":\n",
    "                y, sr = lr.load(file_path)\n",
    "                features, _ =get_mfcc(y,sr, n_fft=n_fft, hop_length=hop_length)\n",
    "                stats=[compute_stats(features[i, :]) for i in range(features.shape[0])] \n",
    "                features_combined=np.concatenate([stats, features], axis=1)\n",
    "                features_combined=np.array(torch.from_numpy(features_combined).permute(1,0))\n",
    "                filename = file.replace(\".wav\", \"_\")\n",
    "                file_class = get_class(filename)\n",
    "\n",
    "                f = [features_combined]\n",
    "                print(features_combined.shape)\n",
    "\n",
    "                filename = file.replace(\".wav\", \"_\")\n",
    "                file_class = get_class(filename)\n",
    "                idx=0\n",
    "                for _f in f:\n",
    "                    filename += str(idx)\n",
    "                    idx+=1\n",
    "                    h5spec = test_dataset.create_dataset(filename, data=_f)\n",
    "                    h5spec.attrs[\"class_label\"] = file_class\n",
    "    test_dataset.close()\n",
    "    print(\"Train, valid and test dataset created\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft=400\n",
    "hop_length=200\n",
    "\n",
    "vers=f\"BerlinEMO_DB_spectral_features_with_timestats_n_fft_{n_fft}_hop_{hop_length}_sr16.txt\"\n",
    "versh5=f\"BerlinEMO_DB_spectral_features_with_timestatse_n_fft_{n_fft}_hop_{hop_length}_sr16.h5\"\n",
    "\n",
    "trainset=\"train_\"+versh5\n",
    "testset=\"test_\"+versh5\n",
    "validset=\"valid_\"+versh5\n",
    "print(trainset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1=os.path.join(os.getcwd(), trainset)\n",
    "if os.path.isfile(file_path1):\n",
    "    os.remove(file_path1)\n",
    "\n",
    "file_path2=os.path.join(os.getcwd(), testset)\n",
    "if os.path.isfile(file_path2):\n",
    "    os.remove(file_path2)\n",
    "\n",
    "file_path3=os.path.join(os.getcwd(), validset)\n",
    "if os.path.isfile(file_path3):\n",
    "    os.remove(file_path3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=os.listdir(audio_data_path_with_silence)\n",
    "random.shuffle(file_list)\n",
    "\n",
    "train_split=0.8\n",
    "valid_test_split=(100-train_split*100)/2*0.01\n",
    "train_len = int(train_split * len(file_list))\n",
    "valid_len = int(valid_test_split * len(file_list))\n",
    "\n",
    "train_list = file_list[:train_len]\n",
    "valid_list = file_list[train_len:train_len+valid_len]\n",
    "test_list = file_list[train_len+valid_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_features_dataset(versh5, audio_data_path_with_silence, train_list, valid_list, test_list, n_fft=n_fft, hop_length=hop_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
