{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARFF to HDF5 Conversion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook processes ARFF files extracted from the Berlin Database of Emotional Speech using OpenSMILE and converts them into HDF5 format for training. It splits the data into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/henry/Desktop/Programming/Thesis/emotion_speech_classification.nosync/models/Open_Smile_Features/config/compare16/ComParE_2016.conf\n"
     ]
    }
   ],
   "source": [
    "import arff\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import shutil\n",
    "import subprocess\n",
    "import shlex\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "arff_folder_path=\"\"\n",
    "silb_data_path=\"\"\n",
    "audio_data_path=\"\"\n",
    "save_folder_path=\"\"\n",
    "\n",
    "config_path=\"\"\n",
    "print(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation Function\n",
    "\n",
    "Iterates through ARFF files in a specified folder.\n",
    "Extracts class labels from filenames (EmoDB convention).\n",
    "Splits data into training (90%) and testing (10%) sets.\n",
    "Reads ARFF data, handles missing values, and reshapes it.\n",
    "Saves the features and labels into separate HDF5 files (train_*.h5 and test_*.h5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_with_arff(dataset_name, folder_path):\n",
    "    def get_class(filename):\n",
    "        if \"W\" in filename[5]:\n",
    "            return 0\n",
    "        if \"L\" in filename[5]:\n",
    "            return 1\n",
    "        if \"E\" in filename[5]:\n",
    "            return 2\n",
    "        if \"A\" in filename[5]:\n",
    "            return 3\n",
    "        if \"F\" in filename[5]:\n",
    "            return 4\n",
    "        if \"T\" in filename[5]:\n",
    "            return 5\n",
    "        if \"N\" in filename[5]:\n",
    "            return 6\n",
    "    # assert dataset_name[-3:]!=\".h5\"\n",
    "    list_files=os.listdir(folder_path)\n",
    "    num_files = len(list_files)\n",
    "    num_train = int(num_files * 0.9)  \n",
    "    training_list = random.sample(list_files, num_train)\n",
    "    testing_list = [file for file in list_files if file not in training_list]\n",
    "    train_dataset = h5py.File((\"train_\"+dataset_name), mode=\"w\", libver=\"latest\")\n",
    "    for file in training_list:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_path[-4:]==\"arff\":\n",
    "                with open(file_path, 'r') as _file:\n",
    "                    arff_data = arff.load(_file)\n",
    "                    data=np.array(arff_data['data'])                \n",
    "                filename = str(file).replace(\".arff\", \"\")\n",
    "                data_series = pd.Series(data[0]) \n",
    "                data_series = pd.to_numeric(data_series, errors='coerce') \n",
    "                data_series.fillna(0, inplace=True)\n",
    "                print(data_series.shape)\n",
    "                data_series=np.array(data_series).reshape(1,90)\n",
    "                # data_series = np.array(data_series).reshape(1, -1)\n",
    "                h5spec = train_dataset.create_dataset(filename, data=data_series)\n",
    "                h5spec.attrs[\"class_label\"] = get_class(filename)                    \n",
    "    train_dataset.close()\n",
    "    test_dataset = h5py.File((\"test_\"+dataset_name), mode=\"w\", libver=\"latest\")\n",
    "    for file in testing_list:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_path[-4:]==\"arff\":\n",
    "                with open(file_path, 'r') as _file:\n",
    "                    arff_data = arff.load(_file)\n",
    "                    data=np.array(arff_data['data']) \n",
    "                data_series = pd.Series(data[0]) \n",
    "                data_series = pd.to_numeric(data_series, errors='coerce') \n",
    "                data_series.fillna(0, inplace=True)\n",
    "                data_series=np.array(data_series).reshape(1,90)\n",
    "                filename = file.replace(\".arff\", \"\")\n",
    "                h5spec = test_dataset.create_dataset(filename, data=data_series)\n",
    "                h5spec.attrs[\"class_label\"] = get_class(filename) \n",
    "    test_dataset.close()\n",
    "    print(\"Train and test dataset created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Sets up the file paths and calls the create_dataset_with_arff function to generate the datasets. It also handles the removal of existing HDF5 files to prevent conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=\"../data/EmoDB/arff_files/\"\n",
    "\n",
    "dataset=f\"EmoDB_eGeMaps.h5\"\n",
    "trainset=\"train_\"+dataset\n",
    "testset=\"test_\"+dataset\n",
    "\n",
    "file_path1=os.path.join(os.getcwd(), trainset)\n",
    "if os.path.isfile(file_path1):\n",
    "    os.remove(file_path1)\n",
    "\n",
    "file_path2=os.path.join(os.getcwd(), testset)\n",
    "if os.path.isfile(file_path2):\n",
    "    os.remove(file_path2)\n",
    "\n",
    "create_dataset_with_arff(dataset, folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
